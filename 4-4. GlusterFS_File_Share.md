GlusterFS 서버 설치
=============
rocky1, rocky2 에서 GlusterFS 서버 구축 후 Ubuntu에서 Mount
<br/> 각 서버는 6기가의 추가 디스크가 있음
<br/>
<br/> GlusterFS 에서 사용되는 복제 및 볼륨의 분산 구성과 같은 기능이
<br/> DNS이름 및 FQDN(Fully Qualified Domain Name)에 의존하기 때문에 hosts 파일 편집이 필요.
<br/>rocky1 IP 주소는 192.168.0.68 hostname server1
<br/>rocky2 IP 주소는 192.168.0.69 hostname server2
<br/> 각 서버 /etc/hosts 파일 수정 (rocky1, rocky2, client)
<br/> 192.168.0.68 server1
<br/> 192.168.0.69 server2
```
1. hostname 변경
2. 디스크 추가 및 마운트
3. GlusterFS 설치
4. GlusterFS 방화벽 등록
5. GlusterFS 클러스터 초기화
6. GlusterFS 볼륨 생성
7. 클라이언트 시스템에 GlusterFS 볼륨 마운트
8. 고가용성 테스트
```

1.&nbsp;hostname 변경
--------------------------
#### hostnamectl set-hostname [호스트명]
```
1-1. rocky1 hostname 변경
[root@localhost /]# hostnamectl set-hostname server1
[root@localhost /]# hostname
server1

1-2. rocky2 hostname 변경
[root@localhost ~]# hostnamectl set-hostname server2
[root@localhost ~]# hostname
server2
```

2.&nbsp;디스크 추가 및 마운트
--------------------------
#### 추가 디스크 /data/vol1 디렉터리에 마운트
```
2-1. 6G 추가 디스크 확인
[root@server1 ~]# fdisk -l
Disk /dev/sde: 6 GiB, 6442450944 bytes, 12582912 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes

2-2. LVM 디스크 생성
[root@server1 ~]# pvcreate /dev/sde
  Physical volume "/dev/sde" successfully created.
[root@server1 ~]# vgcreate vg_data /dev/sde
  Volume group "vg_data" successfully created
[root@server1 ~]# lvcreate -l 100%FREE -n lv_data vg_data
  Logical volume "lv_data" created.
[root@server1 ~]# mkfs.xfs /dev/vg_data/lv_data
meta-data=/dev/vg_data/lv_data   isize=512    agcount=4, agsize=392960 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1    bigtime=0 inobtcount=0
data     =                       bsize=4096   blocks=1571840, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none     

2-3. /data/vol1 디렉터리 생성 및 마운트
[root@server1 /]# mkdir -p /data/vol1
[root@server1 /]# vi /etc/fstab
/dev/mapper/vg_data-lv_data     /data/vol1      xfs     defaults        0       0

[root@server1 /]# mount -a

2-4. 마운트 확인
[root@server1 /]# df -hT
/dev/mapper/vg_data-lv_data                                                                     xfs       6.0G   76M  6.0G   2% /data/vol1

2-5. GlusterFS 에 사용할 brick0 디렉터리 생성
[root@server1 /]# mkdir /data/vol1/brick0

rocky2 서버에도 동일하게 마운트 한다
디렉터리는 /data/vol2로 생성한다.(하위 디렉터리인 brick0는 동일하게)
```
3.&nbsp;GlusterFS 설치
--------------------------
#### GlusterFS 레포지터리 추가, 저장소 편집, 패키지 설치 
```
3-1. centos gluster9 레포지터리 추가
[root@server1 /]# dnf install centos-release-gluster9
Last metadata expiration check: 3:26:00 ago on Mon 01 May 2023 07:32:40 PM KST.
Dependencies resolved.
==============================================================================================================
 Package                                   Architecture       Version                Repository          Size
==============================================================================================================
Installing:
 centos-release-gluster9                   noarch             1.0-1.el8              extras             8.2 k
Installing dependencies:
 centos-release-storage-common             noarch             2-2.el8                extras             8.5 k
 
 ========================== 중간생략 ==========================
 
 Installed:
  centos-release-gluster9-1.0-1.el8.noarch            centos-release-storage-common-2-2.el8.noarch

Complete!

3-2. 저장소 편집
[root@server1 /]# vi /etc/yum.repos.d/CentOS-Gluster-9.repo.

[centos-gluster9]
name=CentOS-$releasever - Gluster 9
#mirrorlist=http://mirrorlist.centos.org?arch=$basearch&release=$releasever&repo=storage-gluster-9 <- 주석처리
baseurl=https://dl.rockylinux.org/vault/centos/8.5.2111/storage/x86_64/gluster-9/ <- 주석해제
gpgcheck=1
enabled=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-Storage

wq로 저장 및 종료

3-3. 저장소 확인
[root@server1 /]# dnf repolist
repo id                                           repo name
appstream                                         Rocky Linux 8 - AppStream
baseos                                            Rocky Linux 8 - BaseOS
centos-gluster9                                   CentOS-8 - Gluster 9
extras                                            Rocky Linux 8 - Extras
 
 
 3-4. GlusterFS 패키지 설치

[root@server1 /]# dnf install glusterfs glusterfs-libs glusterfs-server
CentOS-8 - Gluster 9                                                           20 kB/s |  57 kB     00:02
Last metadata expiration check: 0:00:01 ago on Mon 01 May 2023 11:02:14 PM KST.
Dependencies resolved.
==============================================================================================================
 Package                            Architecture     Version                  Repository                 Size
==============================================================================================================
Installing:
 glusterfs                          x86_64           9.4-1.el8                centos-gluster9           691 k
 glusterfs-server                   x86_64           9.4-1.el8                centos-gluster9           1.4 M
Installing dependencies:
 
 ========================== 중간생략 ==========================
 
 Installed:
  glusterfs-9.4-1.el8.x86_64                               glusterfs-cli-9.4-1.el8.x86_64
  glusterfs-client-xlators-9.4-1.el8.x86_64                glusterfs-fuse-9.4-1.el8.x86_64
  glusterfs-selinux-2.0.1-1.el8.noarch                     glusterfs-server-9.4-1.el8.x86_64
  libgfapi0-9.4-1.el8.x86_64                               libgfchangelog0-9.4-1.el8.x86_64
  libgfrpc0-9.4-1.el8.x86_64                               libgfxdr0-9.4-1.el8.x86_64
  libglusterd0-9.4-1.el8.x86_64                            libglusterfs0-9.4-1.el8.x86_64
  liburing-1.0.7-3.el8.x86_64                              python3-pyxattr-0.5.3-18.el8.x86_64

Complete!

rocky2 서버에도 동일하게 패키지를 설치한다.
```

4.&nbsp;GlusterFS 방화벽 등록
--------------------------
#### firewall-cmd -add-service=glusterfs --permanent
```
[root@server1 /]# firewall-cmd --add-service=glusterfs --permanent
success

서비스명으로 등록 혹은

[root@server1 /]# firewall-cmd --zone=public --add-port=24007-24008/tcp --permanent
success
[root@server1 /]# firewall-cmd --zone=public --add-port=49152/tcp --permanent
success
포트로 등록

[root@server1 /]# firewall-cmd --reload
success

rocky2 서버에도 동일하게 방화벽 

```
5.&nbsp;GlusterFS 클러스터 초기화
--------------------------
#### gluster peer 등록
#### 명령은 단일 노드(여기서는 rocky1서버)에서 한 번만 수행

```
5-1. 등록
[root@server1 ~]# gluster peer probe server2
peer probe: success

5-2. 상태확인
[root@server1 ~]# gluster peer status
Number of Peers: 1

Hostname: server2
Uuid: 5e56946d-d874-4fa8-b85e-43caeff61b31
State: Peer in Cluster (Connected)

5-3. 서버2에서의 상태 확인
[root@server2 ~]# gluster peer status
Number of Peers: 1

Hostname: server1
Uuid: 8dbaadce-4c75-4698-9d4f-1f6dba6d4387
State: Peer in Cluster (Connected)

```

6.&nbsp;GlusterFS 볼륨 생성
--------------------------
#### gluster volume create 명령어를 사용하여 gluster 볼륨 생성
```
6-1. volume1 라는 이름의 gluster 볼륨 생성
[root@server1 ~]# gluster volume create volume1 replica 2 server1:/data/vol1/brick0 server2:/data/vol2/brick0
Replica 2 volumes are prone to split-brain. Use Arbiter or Replica 3 to avoid this. See: http://docs.gluster.org/en/latest/Administrator%20Guide/Split%20brain%20and%20ways%20to%20deal%20with%20it/.
Do you still want to continue?
 (y/n) y
volume create: volume1: success: please start the volume to access data
[root@server1 ~]#

[root@server1 ~]# gluster volume start volume1
volume start: volume1: success



6-2. 볼륨 상태 확인
[root@server1 ~]# gluster volume status
Status of volume: volume1
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick server1:/data/vol1/brick0             49152     0          Y       6352
Brick server2:/data/vol2/brick0             49152     0          Y       6330
Self-heal Daemon on localhost               N/A       N/A        Y       6369
Self-heal Daemon on server2                 N/A       N/A        Y       6347

Task Status of Volume volume1
------------------------------------------------------------------------------
There are no active volume tasks

6-3. 볼륨 정보 확인
[root@server1 ~]# gluster volume info

Volume Name: volume1
Type: Replicate
Volume ID: 1c851029-dd67-4347-b2e0-e1268dc6ed7e
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: server1:/data/vol1/brick0
Brick2: server2:/data/vol2/brick0
Options Reconfigured:
cluster.granular-entry-heal: on
storage.fips-mode-rchecksum: on
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off
```

7.&nbsp;클라이언트 시스템에 GlusterFS 볼륨 마운트
--------------------------
#### Ubuntu 서버에 gluster 패키지 설치
```
7-1. ubuntu Client에서 glusterfs client 패키지설치
root@server-b:~# apt-get install glusterfs-client
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following additional packages will be installed:
  attr glusterfs-common libgfapi0 libgfchangelog0 libgfrpc0 libgfxdr0
  libglusterd0 libglusterfs0 python3-prettytable python3-wcwidth
The following NEW packages will be installed:
  attr glusterfs-client glusterfs-common libgfapi0 libgfchangelog0 libgfrpc0
  
   ========================== 중간생략 ==========================
  
  Done.
Setting up glusterfs-client (10.1-1) ...
Processing triggers for libc-bin (2.35-0ubuntu3.1) ...
Processing triggers for man-db (2.10.2-1) ...
Scanning processes...
Scanning linux images...

7-2. gluster 서버 mount할 /data 디렉터리 생성
root@server-b:/# mkdir /data

7-3. /data에 mount
root@server-b:/# mount.glusterfs server1:/volume1 /data
root@server-b:/# df -hT
server1:/volume1                  fuse.glusterfs  6.0G  137M  5.9G   3% /data

7-4. /etc/fstab에 등록
root@server-b:/data# vi /etc/fstab
server1:/volume1        /data   glusterfs       defaults,_netdev        0       0

```

8.&nbsp;고가용성 테스트
--------------------------
#### test 파일을 만들고 rocky1 서버를 종료했을때 테스트
```
8-1. client에서 /data 디렉터리에 file1, file2, file3 파일 생성
root@server-b:/# cd /data/
root@server-b:/data# touch file{1..3}
root@server-b:/data# ls
file1  file2  file3

8-2. rocky1 서버 종료
[root@server1 ~]# shutdown -r now

8-2. rocky2 서버에서 gluster 상태 확인
[root@server2 ~]# gluster peer status
Number of Peers: 1

Hostname: server1
Uuid: 8dbaadce-4c75-4698-9d4f-1f6dba6d4387
State: Peer in Cluster (Disconnected)

[root@server2 ~]# gluster volume status
Status of volume: volume1
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick server2:/data/vol2/brick0             49152     0          Y       6330
Self-heal Daemon on localhost               N/A       N/A        Y       6347

Task Status of Volume volume1
------------------------------------------------------------------------------
There are no active volume tasks



8-3. Client에서 생성한 파일 확인
[root@server2 ~]# ls /data/vol2/brick0/
file1  file2  file3
```

